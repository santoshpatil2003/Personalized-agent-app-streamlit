{"docstore/metadata": {"e6eb5348-d59b-42fa-89fc-572b46e016d2": {"doc_hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d"}, "1ce4343f-6d9e-4ad2-a022-43fb350e9762": {"doc_hash": "4cf8983c895a9305d5e4845cb9d298eef6cb09e88ebe63ecda0cb36082519235", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "24ae07c8-5092-4deb-a4d8-ec80b6e9964d": {"doc_hash": "4a5e8cc5944b19ed1fc0f3342bfa491c06918583e31cbf84606c133677bfca50", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "e53633a3-f9a4-4305-857c-6411e1abccc3": {"doc_hash": "0300db2b824c9b1e7284a7d96150b93ee7f288cf98216df33de6538edbb19151", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "2fcd423d-6a44-4356-bc7d-c89f4a205298": {"doc_hash": "d43e7ce838ec4bdb1404edb8bd1d6a9210d5d20c52ac68c62ed358be1768d1c3", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "71d7c476-0afe-40a1-a3c4-a3fda89c9284": {"doc_hash": "0ad69e4e4d75eb713609fea99a00dc606795e0600a583cfd25c029d1d6e288ca", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "e488104f-b966-4ebb-b47e-28a934adab0b": {"doc_hash": "9fc86f3db5a2bd40c9887dd6299ce5a406f78833b91acdb5116dbdcbe064af9c", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "3028c46c-bf70-451a-af78-07dc4ff01092": {"doc_hash": "8a59226af5fb31020d8748ba358503c111ff4eb2915d696fd6041bb93f80b618", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "fbe46a8d-a029-42ff-8a1b-e7856667517e": {"doc_hash": "087b80027dc29928fa181c5b73f068c1f337bdcf5d33f06f701e1470b006fe4b", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "d1f8c92f-a7df-4517-96dd-b1ecb792edee": {"doc_hash": "7cdbc43d093b9eed425d282dbbaee190bab744f2f086930ff01f4c834f85d06c", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "fb0b2d25-ff0f-4b9a-ac0b-94d3df42dc27": {"doc_hash": "2b53c767ef011a674794804880d5b110471226467d4bf665ecae7ba60656ddcc", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "1fdad8eb-2931-4437-9b07-94872518ca75": {"doc_hash": "f3600f87b5a4ec4f4104453fe4ea00af86fafc68629a668c1af9112ed6dc731e", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "f45c7b04-5b42-4598-97df-c227cf12ed14": {"doc_hash": "464a981c7e1aba4953b3c47ca8e724369603955706f6b35e8377346c840bfb38", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}, "311de25e-191a-487a-9409-5fcbe975499a": {"doc_hash": "437a81edd526de65d329cd3080919136d0ad6df746a50bac230c251a27067086", "ref_doc_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2"}}, "docstore/data": {"1ce4343f-6d9e-4ad2-a022-43fb350e9762": {"__data__": {"id_": "1ce4343f-6d9e-4ad2-a022-43fb350e9762", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24ae07c8-5092-4deb-a4d8-ec80b6e9964d", "node_type": "1", "metadata": {}, "hash": "ce41acad60410623fd3c85b32e160758765507477d8a2c0ef2182f61ea1dce22", "class_name": "RelatedNodeInfo"}}, "text": "LangChain v/s LlamaIndex \u2014 How do they compare?\r\n\r\n\r\n I stated my support for Retrieval-Augmented Generation (RAG) as the key technology of private, offline, decentralized LLM applications. When you build something for your own use, you\u2019re fighting alone. You can build from scratch, but it would be more efficient to build upon an existing framework.\r\n\r\nAFAIK, two choices exist, aiming at different scopes:\r\n\r\nLangChain, a generic framework for developing stuff with LLM.\r\nLlamaIndex, a framework dedicated for building RAG systems.\r\nPicking a framework is a big investment. You want one that enjoys strong maintainers and vibrant communities. Fortunately, both choices have incorporated last year, so the sizes are quite quantifiable. Here\u2019s how the numbers compare:\r\n\r\n\r\nJudging from the financials, LlamaIndex is coming strong with a funding amount close to that of LangChain although their target market is much smaller (using GitHub stars as an approximate of community interest). This might indicate better chance of survival for LlamaIndex. That being said, LangChain offers more enterprise-oriented products that can generate revenue (LangServe, LangSmith, \u2026), so the argument may be reversed. It\u2019s a tough call from the monetary perspective.\r\n\r\nMy Finance 101 could only take me this far. Let\u2019s get to what I\u2019m actually good at and talk in Python. In this article, I\u2019m going to complete some basic tasks with both frameworks in parallel. By presenting the code snippets side-by-side, I hope it helps you make a more informed decision on which to employ in your own RAG chatbot.\r\n\r\nCreating a chatbot with a local LLM\r\nFor the first task to implement, I chose making a local-only chatbot. This is because I don\u2019t want to pay a cloud service for mock chat messages while learning to use these frameworks.\r\n\r\nI chose to keep a LLM running in a standalone inference server, instead of having the frameworks load the multi-gigabyte model into the memory every time I run the scripts. This saves time and avoids wearing down the disk.\r\n\r\nWhile there are multiple API schema for LLM inference, I chose one that is OpenAI-compatible, so that it most closely resembles the official OpenAI endpoint, should you want to.", "start_char_idx": 0, "end_char_idx": 2220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24ae07c8-5092-4deb-a4d8-ec80b6e9964d": {"__data__": {"id_": "24ae07c8-5092-4deb-a4d8-ec80b6e9964d", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ce4343f-6d9e-4ad2-a022-43fb350e9762", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "4cf8983c895a9305d5e4845cb9d298eef6cb09e88ebe63ecda0cb36082519235", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e53633a3-f9a4-4305-857c-6411e1abccc3", "node_type": "1", "metadata": {}, "hash": "abc25e75186ecb72a8209aacf77dd98a494645449d28cfda326c9826260b0cbc", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s get to what I\u2019m actually good at and talk in Python. In this article, I\u2019m going to complete some basic tasks with both frameworks in parallel. By presenting the code snippets side-by-side, I hope it helps you make a more informed decision on which to employ in your own RAG chatbot.\r\n\r\nCreating a chatbot with a local LLM\r\nFor the first task to implement, I chose making a local-only chatbot. This is because I don\u2019t want to pay a cloud service for mock chat messages while learning to use these frameworks.\r\n\r\nI chose to keep a LLM running in a standalone inference server, instead of having the frameworks load the multi-gigabyte model into the memory every time I run the scripts. This saves time and avoids wearing down the disk.\r\n\r\nWhile there are multiple API schema for LLM inference, I chose one that is OpenAI-compatible, so that it most closely resembles the official OpenAI endpoint, should you want to.\r\n\r\nThis is how you would do it with LlamaIndex:\r\n\r\nfrom llama_index.llms import ChatMessage, OpenAILike  \r\n  \r\nllm = OpenAILike(  \r\n    api_base=\"http://localhost:1234/v1\",  \r\n    timeout=600,  # secs  \r\n    api_key=\"loremIpsum\",  \r\n    is_chat_model=True,  \r\n    context_window=32768,  \r\n)  \r\nchat_history = [  \r\n    ChatMessage(role=\"system\", content=\"You are a bartender.\"),  \r\n    ChatMessage(role=\"user\", content=\"What do I enjoy drinking?\"),  \r\n]  \r\noutput = llm.chat(chat_history)  \r\nprint(output)\r\n\r\n\r\nAnd this is LangChain:\r\n\r\nfrom langchain.schema import HumanMessage, SystemMessage  \r\nfrom langchain_openai import ChatOpenAI  \r\n  \r\nllm = ChatOpenAI(  \r\n    openai_api_base=\"http://localhost:1234/v1\",  \r\n    request_timeout=600,  # secs, I guess.  \r\n    openai_api_key=\"loremIpsum\",  \r\n    max_tokens=32768,  \r\n)  \r\nchat_history = [  \r\n    SystemMessage(content=\"You are a bartender.\"),  \r\n    HumanMessage(content=\"What do I enjoy drinking?\"),  \r\n]  \r\nprint(llm(chat_history))\r\n\r\n\r\nWith both, the API key can be arbitrary, but it must present.", "start_char_idx": 1300, "end_char_idx": 3276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e53633a3-f9a4-4305-857c-6411e1abccc3": {"__data__": {"id_": "e53633a3-f9a4-4305-857c-6411e1abccc3", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24ae07c8-5092-4deb-a4d8-ec80b6e9964d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "4a5e8cc5944b19ed1fc0f3342bfa491c06918583e31cbf84606c133677bfca50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fcd423d-6a44-4356-bc7d-c89f4a205298", "node_type": "1", "metadata": {}, "hash": "fb0c777a19b2f8becff9174262108bd3d35114e439d499a74237801610a82a75", "class_name": "RelatedNodeInfo"}}, "text": "\"),  \r\n    ChatMessage(role=\"user\", content=\"What do I enjoy drinking?\"),  \r\n]  \r\noutput = llm.chat(chat_history)  \r\nprint(output)\r\n\r\n\r\nAnd this is LangChain:\r\n\r\nfrom langchain.schema import HumanMessage, SystemMessage  \r\nfrom langchain_openai import ChatOpenAI  \r\n  \r\nllm = ChatOpenAI(  \r\n    openai_api_base=\"http://localhost:1234/v1\",  \r\n    request_timeout=600,  # secs, I guess.  \r\n    openai_api_key=\"loremIpsum\",  \r\n    max_tokens=32768,  \r\n)  \r\nchat_history = [  \r\n    SystemMessage(content=\"You are a bartender.\"),  \r\n    HumanMessage(content=\"What do I enjoy drinking?\"),  \r\n]  \r\nprint(llm(chat_history))\r\n\r\n\r\nWith both, the API key can be arbitrary, but it must present. I guess it\u2019s a requirement from the OpenAI SDK that is running under the hood in both frameworks.\r\n\r\nLangChain distinguishes between chat-able LLMs (ChatOpenAI) and completion-only LLMs (OpenAI), while LlamaIndex controls it with a is_chat_model parameter in the constructor.\r\nLlamaIndex distinguishes official OpenAI endpoints and OpenAILike endpoints, while LangChain determines where to send requests to via a openai_api_base parameter.\r\nWhile LlamaIndex labels chat messages with the role parameter, LangChain uses separate classes.\r\nSo far, things didn\u2019t look very different across the two frameworks. Let\u2019s carry on.\r\n\r\nBuilding a RAG system for local files\r\nWith a LLM connected, we can get to business. Let\u2019s now build a simple RAG system that reads from a local folder of text files. Here\u2019s how to achieve that with LlamaIndex, taken largely from this documentation:\r\n\r\nfrom llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\r\n\r\nservice_context = ServiceContext.from_defaults(  \r\n    embed_model=\"local\",  \r\n    llm=llm, # This should be the LLM initialized in the task above.\r\n)", "start_char_idx": 2595, "end_char_idx": 4389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fcd423d-6a44-4356-bc7d-c89f4a205298": {"__data__": {"id_": "2fcd423d-6a44-4356-bc7d-c89f4a205298", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e53633a3-f9a4-4305-857c-6411e1abccc3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "0300db2b824c9b1e7284a7d96150b93ee7f288cf98216df33de6538edbb19151", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71d7c476-0afe-40a1-a3c4-a3fda89c9284", "node_type": "1", "metadata": {}, "hash": "00edf89bd3a68f449bda92983896fc58013bd666e2181e5bd3adcca1f9713ea4", "class_name": "RelatedNodeInfo"}}, "text": "LlamaIndex distinguishes official OpenAI endpoints and OpenAILike endpoints, while LangChain determines where to send requests to via a openai_api_base parameter.\r\nWhile LlamaIndex labels chat messages with the role parameter, LangChain uses separate classes.\r\nSo far, things didn\u2019t look very different across the two frameworks. Let\u2019s carry on.\r\n\r\nBuilding a RAG system for local files\r\nWith a LLM connected, we can get to business. Let\u2019s now build a simple RAG system that reads from a local folder of text files. Here\u2019s how to achieve that with LlamaIndex, taken largely from this documentation:\r\n\r\nfrom llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\r\n\r\nservice_context = ServiceContext.from_defaults(  \r\n    embed_model=\"local\",  \r\n    llm=llm, # This should be the LLM initialized in the task above.\r\n)  \r\ndocuments = SimpleDirectoryReader(\r\n    input_dir=\"mock_notebook/\",\r\n).load_data()  \r\nindex = VectorStoreIndex.from_documents(  \r\n    documents=documents,\r\n    service_context=service_context,\r\n)\r\nengine = index.as_query_engine(  \r\n    service_context=service_context,  \r\n)\r\noutput = engine.query(\"What do I like to drink?\")", "start_char_idx": 3554, "end_char_idx": 4717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71d7c476-0afe-40a1-a3c4-a3fda89c9284": {"__data__": {"id_": "71d7c476-0afe-40a1-a3c4-a3fda89c9284", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fcd423d-6a44-4356-bc7d-c89f4a205298", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "d43e7ce838ec4bdb1404edb8bd1d6a9210d5d20c52ac68c62ed358be1768d1c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e488104f-b966-4ebb-b47e-28a934adab0b", "node_type": "1", "metadata": {}, "hash": "aa67eb302319d9fa3e0a1c559dd652317d759726f4aab2957c2087a138b40790", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s carry on.\r\n\r\nBuilding a RAG system for local files\r\nWith a LLM connected, we can get to business. Let\u2019s now build a simple RAG system that reads from a local folder of text files. Here\u2019s how to achieve that with LlamaIndex, taken largely from this documentation:\r\n\r\nfrom llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\r\n\r\nservice_context = ServiceContext.from_defaults(  \r\n    embed_model=\"local\",  \r\n    llm=llm, # This should be the LLM initialized in the task above.\r\n)  \r\ndocuments = SimpleDirectoryReader(\r\n    input_dir=\"mock_notebook/\",\r\n).load_data()  \r\nindex = VectorStoreIndex.from_documents(  \r\n    documents=documents,\r\n    service_context=service_context,\r\n)\r\nengine = index.as_query_engine(  \r\n    service_context=service_context,  \r\n)\r\noutput = engine.query(\"What do I like to drink?\")  \r\nprint(output)\r\nWith LangChain, number of lines would double, but it would still be manageable:\r\n\r\nfrom langchain_community.document_loaders import DirectoryLoader  \r\n  \r\n# pip install \"unstructured[md]\"  \r\nloader = DirectoryLoader(\"mock_notebook/\", glob=\"*.md\")  \r\ndocs = loader.load()  \r\n  \r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter  \r\n  \r\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)  \r\nsplits = text_splitter.split_documents(docs)  \r\n  \r\nfrom langchain_community.embeddings.fastembed import FastEmbedEmbeddings  \r\nfrom langchain_community.vectorstores import Chroma  \r\n  \r\nvectorstore = Chroma.from_documents(documents=splits, embedding=FastEmbedEmbeddings())  \r\nretriever = vectorstore.as_retriever()  \r\n  \r\nfrom langchain import hub  \r\n  \r\n# pip install langchainhub  \r\nprompt = hub.pull(\"rlm/rag-prompt\")  \r\n  \r\n  \r\ndef format_docs(docs):  \r\n    return \"\\n\\n\".join(doc.page_content for doc in docs)  \r\n  \r\n  \r\nfrom langchain_core.runnables import RunnablePassthrough  \r\n  \r\nrag_chain = (  \r\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}  \r\n    | prompt  \r\n    | llm # This should be the LLM initialized in the task above.\r\n)  \r\nprint(rag_chain.invoke(\"What do I like to drink?\"))", "start_char_idx": 3884, "end_char_idx": 6000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e488104f-b966-4ebb-b47e-28a934adab0b": {"__data__": {"id_": "e488104f-b966-4ebb-b47e-28a934adab0b", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71d7c476-0afe-40a1-a3c4-a3fda89c9284", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "0ad69e4e4d75eb713609fea99a00dc606795e0600a583cfd25c029d1d6e288ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3028c46c-bf70-451a-af78-07dc4ff01092", "node_type": "1", "metadata": {}, "hash": "f54a4fb43f36810cb9da96180a1fc4e50d2dcefb55f00fe228c170595f213d45", "class_name": "RelatedNodeInfo"}}, "text": "print(rag_chain.invoke(\"What do I like to drink?\"))\r\nThese snippets clearly illustrate the different levels of abstraction across these two frameworks. While LlamaIndex wraps the RAG pipeline with a convenient package called \u201cquery engines\u201d, LangChain exposes you to the inner components. They include the concatenator for retrieved documents, the prompt template that says \u201cbased on X please answer Y\u201d, and the chain itself (shown in LCEL above).\r\n\r\nThis lack of abstraction has implications on learners: When building with LangChain, you have to know exactly what you want on the first try. For example, compare where from_documents is invoked. LlamaIndex allows you to play with a Vector Store Index without explicitly choosing a storage backend, whereas LangChain seems to suggest you pick an implementation right away. (Everybody seems to have explicitly picked a backend when they create Vector Indexes from documents with LangChain.) I\u2019m not sure if I\u2019m making an informed decision when choosing a database before I hit a scalability problem.\r\n\r\nMore interestingly, although both LangChain and LlamaIndex are providing Hugging Face Hub-like cloud services (namely, LangSmith Hub and LlamaHub), it\u2019s LangChain who dialed it to 11. Notice the hub.pull call with LangChain. It downloads nothing but a short, textual template that reads:\r\n\r\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\u2019t know the answer, just say that you don\u2019t know. Use three sentences maximum and keep the answer concise.\r\nQuestion: {question}\r\nContext: {context}\r\nAnswer:\r\n\r\nWhile this indeed encourages sharing eloquent prompts with the community, I feel it\u2019s an overkill. Storing ~1kB of text doesn\u2019t really justify the network call involved with pulling. I hope the downloaded artifacts are cached.\r\n\r\nCombining the two: a RAG-enabled chatbot\r\nUp till now, we\u2019ve been building things that aren\u2019t very smart. In the first task, we built something that can maintain a conversation but doesn\u2019t know you very well; in the second, we built something that knows you but doesn\u2019t retain a chat history. Let\u2019s combine these two.", "start_char_idx": 5949, "end_char_idx": 8132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3028c46c-bf70-451a-af78-07dc4ff01092": {"__data__": {"id_": "3028c46c-bf70-451a-af78-07dc4ff01092", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e488104f-b966-4ebb-b47e-28a934adab0b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "9fc86f3db5a2bd40c9887dd6299ce5a406f78833b91acdb5116dbdcbe064af9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbe46a8d-a029-42ff-8a1b-e7856667517e", "node_type": "1", "metadata": {}, "hash": "9b47627cf929da9f18d43655343048f02391c403f0b5f2f0a14fba3c2196e182", "class_name": "RelatedNodeInfo"}}, "text": "It downloads nothing but a short, textual template that reads:\r\n\r\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\u2019t know the answer, just say that you don\u2019t know. Use three sentences maximum and keep the answer concise.\r\nQuestion: {question}\r\nContext: {context}\r\nAnswer:\r\n\r\nWhile this indeed encourages sharing eloquent prompts with the community, I feel it\u2019s an overkill. Storing ~1kB of text doesn\u2019t really justify the network call involved with pulling. I hope the downloaded artifacts are cached.\r\n\r\nCombining the two: a RAG-enabled chatbot\r\nUp till now, we\u2019ve been building things that aren\u2019t very smart. In the first task, we built something that can maintain a conversation but doesn\u2019t know you very well; in the second, we built something that knows you but doesn\u2019t retain a chat history. Let\u2019s combine these two.\r\n\r\nWith LlamaIndex, it\u2019s as simple as swapping as_query_engine with as_chat_engine:\r\n\r\n# Everything from above, till and including the creation of the index.\r\nengine = index.as_chat_engine()\r\noutput = engine.chat(\"What do I like to drink?\")  \r\nprint(output) # \"You enjoy drinking coffee.\"\r\noutput = engine.chat(\"How do I brew it?\")  \r\nprint(output) # \"You brew coffee with a Aeropress.\"\r\nWith LangChain, we need to spell things out quite a bit. Following the official tutorial, let\u2019s define the memory first:\r\n\r\n# Everything above this line is the same as that of the last task.\r\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda  \r\nfrom langchain_core.messages import get_buffer_string  \r\nfrom langchain_core.output_parsers import StrOutputParser  \r\nfrom operator import itemgetter  \r\nfrom langchain.memory import ConversationBufferMemory  \r\nfrom langchain.prompts.prompt import PromptTemplate  \r\nfrom langchain.schema import format_document  \r\nfrom langchain_core.prompts import ChatPromptTemplate  \r\n  \r\nmemory = ConversationBufferMemory(  \r\n    return_messages=True, output_key=\"answer\", input_key=\"question\"  \r\n)\r\n\r\nHere\u2019s the plan:\r\n\r\nAt the start of LLM\u2019s turn, we load the chat history from the memory.", "start_char_idx": 7227, "end_char_idx": 9366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbe46a8d-a029-42ff-8a1b-e7856667517e": {"__data__": {"id_": "fbe46a8d-a029-42ff-8a1b-e7856667517e", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3028c46c-bf70-451a-af78-07dc4ff01092", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "8a59226af5fb31020d8748ba358503c111ff4eb2915d696fd6041bb93f80b618", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1f8c92f-a7df-4517-96dd-b1ecb792edee", "node_type": "1", "metadata": {}, "hash": "df3d59e69134942f373ada5af8583ba68d7a03f95956f2349c4ee1ccf71557dc", "class_name": "RelatedNodeInfo"}}, "text": "output = engine.chat(\"How do I brew it?\")  \r\nprint(output) # \"You brew coffee with a Aeropress.\"\r\nWith LangChain, we need to spell things out quite a bit. Following the official tutorial, let\u2019s define the memory first:\r\n\r\n# Everything above this line is the same as that of the last task.\r\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda  \r\nfrom langchain_core.messages import get_buffer_string  \r\nfrom langchain_core.output_parsers import StrOutputParser  \r\nfrom operator import itemgetter  \r\nfrom langchain.memory import ConversationBufferMemory  \r\nfrom langchain.prompts.prompt import PromptTemplate  \r\nfrom langchain.schema import format_document  \r\nfrom langchain_core.prompts import ChatPromptTemplate  \r\n  \r\nmemory = ConversationBufferMemory(  \r\n    return_messages=True, output_key=\"answer\", input_key=\"question\"  \r\n)\r\n\r\nHere\u2019s the plan:\r\n\r\nAt the start of LLM\u2019s turn, we load the chat history from the memory.\r\nload_history_from_memory = RunnableLambda(memory.load_memory_variables) | itemgetter(  \r\n    \"history\"  \r\n)  \r\n\r\nload_history_from_memory_and_carry_along = RunnablePassthrough.assign(  \r\n    chat_history=load_history_from_memory  \r\n)\r\n\r\n2. We ask the LLM to enrich the question with context: \u201cTaking the chat history into consideration, what should I look for in my notes to answer this question?\u201d\r\n\r\nrephrase_the_question = (  \r\n    {  \r\n        \"question\": itemgetter(\"question\"),  \r\n        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),  \r\n    }  \r\n    | PromptTemplate.from_template(  \r\n        \"\"\"You're a personal assistant to the user.  \r\nHere's your conversation with the user so far:  \r\n{chat_history}  \r\nNow the user asked: {question}  \r\nTo answer this question, you need to look up from their notes about \"\"\"  \r\n    )  \r\n    | llm  \r\n    | StrOutputParser()  \r\n)\r\n\r\n(We can\u2019t just concatenate the two, because the topics may have changed during the conversation, making most semantic information in the chat log irrelevant.)\r\n3. We run the RAG pipeline. Notice how we have cheated the LLM by implying \u201cwe as the user will be looking up the notes themselves\u201d, but in fact we are asking the LLM to do the heavy lifting now.", "start_char_idx": 8423, "end_char_idx": 10618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1f8c92f-a7df-4517-96dd-b1ecb792edee": {"__data__": {"id_": "d1f8c92f-a7df-4517-96dd-b1ecb792edee", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbe46a8d-a029-42ff-8a1b-e7856667517e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "087b80027dc29928fa181c5b73f068c1f337bdcf5d33f06f701e1470b006fe4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb0b2d25-ff0f-4b9a-ac0b-94d3df42dc27", "node_type": "1", "metadata": {}, "hash": "3cf2f8bb5707d60e25f2c173bb229cd5ca4fe6cb221df1d04bc676e680a9218b", "class_name": "RelatedNodeInfo"}}, "text": "Here's your conversation with the user so far:  \r\n{chat_history}  \r\nNow the user asked: {question}  \r\nTo answer this question, you need to look up from their notes about \"\"\"  \r\n    )  \r\n    | llm  \r\n    | StrOutputParser()  \r\n)\r\n\r\n(We can\u2019t just concatenate the two, because the topics may have changed during the conversation, making most semantic information in the chat log irrelevant.)\r\n3. We run the RAG pipeline. Notice how we have cheated the LLM by implying \u201cwe as the user will be looking up the notes themselves\u201d, but in fact we are asking the LLM to do the heavy lifting now. I feel bad.\r\n\r\nretrieve_documents = {  \r\n    \"docs\": itemgetter(\"standalone_question\") | retriever,  \r\n    \"question\": itemgetter(\"standalone_question\"),  \r\n}\r\n4. We ask the LLM: \u201cTaking the retrieved documents as reference (and \u2014 optionally \u2014 the conversation so far), what would be your response to the user\u2019s latest question?\u201d\r\n\r\ndef _combine_documents(docs):  \r\n    prompt = PromptTemplate.from_template(template=\"{page_content}\")  \r\n    doc_strings = [format_document(doc, prompt) for doc in docs]  \r\n    return \"\\n\\n\".join(doc_strings)  \r\ncompose_the_final_answer = (  \r\n    {  \r\n        \"context\": lambda x: _combine_documents(x[\"docs\"]),  \r\n        \"question\": itemgetter(\"question\"),  \r\n    }  \r\n    | ChatPromptTemplate.from_template(  \r\n        \"\"\"You're a personal assistant.  \r\nWith the context below:  \r\n{context}  \r\nTo the question \"{question}\", you answer:\"\"\"  \r\n    )  \r\n    | llm  \r\n)\r\n\r\n5. We append the final response to the chat history.\r\n\r\n# Putting all 4 stages together...\r\nfinal_chain = (  \r\n    load_history_from_memory_and_carry_along  \r\n    | {\"standalone_question\": rephrase_the_question}  \r\n    | retrieve_documents  \r\n    | compose_the_final_answer  \r\n)  \r\n\r\n# Demo.\r\ninputs = {\"question\": \"What do I like to drink?\"}  \r\noutput = final_chain.invoke(inputs)  \r\nmemory.save_context(inputs, {\"answer\": output.content})  \r\nprint(output) # \"You enjoy drinking coffee.\"\r\ninputs = {\"question\": \"How do I brew it?\"}", "start_char_idx": 10032, "end_char_idx": 12057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb0b2d25-ff0f-4b9a-ac0b-94d3df42dc27": {"__data__": {"id_": "fb0b2d25-ff0f-4b9a-ac0b-94d3df42dc27", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1f8c92f-a7df-4517-96dd-b1ecb792edee", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "7cdbc43d093b9eed425d282dbbaee190bab744f2f086930ff01f4c834f85d06c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fdad8eb-2931-4437-9b07-94872518ca75", "node_type": "1", "metadata": {}, "hash": "d085f60514a8fccf5de9aa67da9c701f2ac7e71361649fb313f84cdf5b3cc2c9", "class_name": "RelatedNodeInfo"}}, "text": "With the context below:  \r\n{context}  \r\nTo the question \"{question}\", you answer:\"\"\"  \r\n    )  \r\n    | llm  \r\n)\r\n\r\n5. We append the final response to the chat history.\r\n\r\n# Putting all 4 stages together...\r\nfinal_chain = (  \r\n    load_history_from_memory_and_carry_along  \r\n    | {\"standalone_question\": rephrase_the_question}  \r\n    | retrieve_documents  \r\n    | compose_the_final_answer  \r\n)  \r\n\r\n# Demo.\r\ninputs = {\"question\": \"What do I like to drink?\"}  \r\noutput = final_chain.invoke(inputs)  \r\nmemory.save_context(inputs, {\"answer\": output.content})  \r\nprint(output) # \"You enjoy drinking coffee.\"\r\ninputs = {\"question\": \"How do I brew it?\"}  \r\noutput = final_chain.invoke(inputs)  \r\nmemory.save_context(inputs, {\"answer\": output.content})  \r\nprint(output) # \"You brew coffee with a Aeropress.\"\r\nThat\u2019s quite a journey! We learned a lot about how a LLM-powered application is usually built. Especially, we exploited the LLM a couple of times by having it assume different persona: a query generator, someone who summarizes retrieved documents, and finally the participant of our conversation. I also hope you got adequately comfortable with the LCEL by now.\r\n\r\nUpgrading to agents\r\nIf you treat the LLM persona that talks to you as a person, the RAG pipeline can be thought of a tool that the person uses. A person can have access to more than one tool, and so can a LLM. You can give it tools for searching Google, looking up Wikipedia, checking weather forecasts, etc. In this way, your chatbot can answer questions about things outside of its immediate knowledge.\r\n\r\nIt doesn\u2019t have to be informational tools. By giving our LLM tools like searching the web, placing some shopping orders, replying to your emails, etc., you can make it capable of affecting the reality and making a difference to the world.\r\n\r\nWith many tools comes the need to decide which ones to use, and in what order. This ability is referred to as \u201cagency\u201d. The persona of your LLM who has agency is thus called an \u201cAgent\u201d.\r\n\r\nThere are multiple ways to give agency to a LLM application.", "start_char_idx": 11410, "end_char_idx": 13477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fdad8eb-2931-4437-9b07-94872518ca75": {"__data__": {"id_": "1fdad8eb-2931-4437-9b07-94872518ca75", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb0b2d25-ff0f-4b9a-ac0b-94d3df42dc27", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "2b53c767ef011a674794804880d5b110471226467d4bf665ecae7ba60656ddcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f45c7b04-5b42-4598-97df-c227cf12ed14", "node_type": "1", "metadata": {}, "hash": "a0335379dfc3313a4a9600a545ff0ea10bdfdfea2967a93a3ad98a07a2b5e6db", "class_name": "RelatedNodeInfo"}}, "text": "A person can have access to more than one tool, and so can a LLM. You can give it tools for searching Google, looking up Wikipedia, checking weather forecasts, etc. In this way, your chatbot can answer questions about things outside of its immediate knowledge.\r\n\r\nIt doesn\u2019t have to be informational tools. By giving our LLM tools like searching the web, placing some shopping orders, replying to your emails, etc., you can make it capable of affecting the reality and making a difference to the world.\r\n\r\nWith many tools comes the need to decide which ones to use, and in what order. This ability is referred to as \u201cagency\u201d. The persona of your LLM who has agency is thus called an \u201cAgent\u201d.\r\n\r\nThere are multiple ways to give agency to a LLM application. The most model-generic (and thus self-host-friendly) way is perhaps the ReAct paradigm, which I wrote a bit more about in the previous post.\r\n\r\nTo do that in LlamaIndex,\r\n\r\n# Everything above this line is the same as in the above two tasks,  \r\n# till and including where `notes_query_engine` is defined.  \r\n# Let's convert the query engine into a tool.  \r\nfrom llama_index.tools import ToolMetadata  \r\nfrom llama_index.tools.query_engine import QueryEngineTool  \r\n  \r\nnotes_query_engine_tool = QueryEngineTool(  \r\n    query_engine=notes_query_engine,  \r\n    metadata=ToolMetadata(  \r\n        name=\"look_up_notes\",  \r\n        description=\"Gives information about the user.\",  \r\n    ),  \r\n)  \r\n\r\nfrom llama_index.agent import ReActAgent  \r\n  \r\nagent = ReActAgent.from_tools(  \r\n    tools=[notes_query_engine_tool],  \r\n    llm=llm,  \r\n    service_context=service_context,  \r\n)  \r\n\r\noutput = agent.chat(\"What do I like to drink?\")  \r\nprint(output) # \"You enjoy drinking coffee.\"\r\noutput = agent.chat(\"How do I brew it?\")  \r\nprint(output) # \"You can use a drip coffee maker, French press, pour-over, or espresso machine.\"\r\nNote that, for our follow-up question \u201chow do I brew coffee\u201d, the agent answered differently from when it was merely a query engine. This is because agents can make their own decision about whether to look up from our notes.", "start_char_idx": 12722, "end_char_idx": 14820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f45c7b04-5b42-4598-97df-c227cf12ed14": {"__data__": {"id_": "f45c7b04-5b42-4598-97df-c227cf12ed14", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fdad8eb-2931-4437-9b07-94872518ca75", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "f3600f87b5a4ec4f4104453fe4ea00af86fafc68629a668c1af9112ed6dc731e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "311de25e-191a-487a-9409-5fcbe975499a", "node_type": "1", "metadata": {}, "hash": "b92a2ab99a6a98d7b1be29a16c327c81506b2500570f90629bcf8758221acaff", "class_name": "RelatedNodeInfo"}}, "text": "\",  \r\n    ),  \r\n)  \r\n\r\nfrom llama_index.agent import ReActAgent  \r\n  \r\nagent = ReActAgent.from_tools(  \r\n    tools=[notes_query_engine_tool],  \r\n    llm=llm,  \r\n    service_context=service_context,  \r\n)  \r\n\r\noutput = agent.chat(\"What do I like to drink?\")  \r\nprint(output) # \"You enjoy drinking coffee.\"\r\noutput = agent.chat(\"How do I brew it?\")  \r\nprint(output) # \"You can use a drip coffee maker, French press, pour-over, or espresso machine.\"\r\nNote that, for our follow-up question \u201chow do I brew coffee\u201d, the agent answered differently from when it was merely a query engine. This is because agents can make their own decision about whether to look up from our notes. If they feel confident enough to answer the question, the agent may choose to not use any tool at all. Our question of \u201chow do I \u2026\u201d can be interpreted both ways: either about generic options, or factual recollections. Apparently, the agent chose to understood it the former way, whereas our query engine (which has a duty of looking up documents from the index) had to pick the latter.\r\n\r\nInterestingly, agents are a use case that LangChain decided to provide a high-level abstraction for:\r\n\r\n# Everything above is the same as in the 2nd task, till and including where we defined `rag_chain`.  \r\n# Let's convert the chain into a tool.  \r\nfrom langchain.agents import AgentExecutor, Tool, create_react_agent  \r\n  \r\ntools = [  \r\n    Tool(  \r\n        name=\"look_up_notes\",  \r\n        func=rag_chain.invoke,  \r\n        description=\"Gives information about the user.\",  \r\n    ),\r\n]\r\n\r\nreact_prompt = hub.pull(\"hwchase17/react-chat\")  \r\nagent = create_react_agent(llm, tools, react_prompt)  \r\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools)  \r\n  \r\nresult = agent_executor.invoke(  \r\n    {\"input\": \"What do I like to drink?\", \"chat_history\": \"\"}  \r\n)  \r\nprint(result) # \"You enjoy drinking coffee.\"\r\nresult = agent_executor.invoke(  \r\n    {  \r\n        \"input\": \"How do I brew it?", "start_char_idx": 14149, "end_char_idx": 16123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "311de25e-191a-487a-9409-5fcbe975499a": {"__data__": {"id_": "311de25e-191a-487a-9409-5fcbe975499a", "embedding": null, "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6eb5348-d59b-42fa-89fc-572b46e016d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "a571a3ad353e34adef34f9b9b5bdf7238daf3299413d628c569c7669fa08041d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f45c7b04-5b42-4598-97df-c227cf12ed14", "node_type": "1", "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}, "hash": "464a981c7e1aba4953b3c47ca8e724369603955706f6b35e8377346c840bfb38", "class_name": "RelatedNodeInfo"}}, "text": "# Let's convert the chain into a tool.  \r\nfrom langchain.agents import AgentExecutor, Tool, create_react_agent  \r\n  \r\ntools = [  \r\n    Tool(  \r\n        name=\"look_up_notes\",  \r\n        func=rag_chain.invoke,  \r\n        description=\"Gives information about the user.\",  \r\n    ),\r\n]\r\n\r\nreact_prompt = hub.pull(\"hwchase17/react-chat\")  \r\nagent = create_react_agent(llm, tools, react_prompt)  \r\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools)  \r\n  \r\nresult = agent_executor.invoke(  \r\n    {\"input\": \"What do I like to drink?\", \"chat_history\": \"\"}  \r\n)  \r\nprint(result) # \"You enjoy drinking coffee.\"\r\nresult = agent_executor.invoke(  \r\n    {  \r\n        \"input\": \"How do I brew it?\",  \r\n        \"chat_history\": \"Human: What do I like to drink?\\nAI: You enjoy drinking coffee.\",  \r\n    }\r\n)\r\n\r\nprint(result) # \"You can use a drip coffee maker, French press, pour-over, or espresso machine.\"\r\nAlthough we still had to manually manage the chat history, it\u2019s much easier to make an agent compared to making a RAG chain. create_react_agent and AgentExecutor cover most of the wiring work under the hood.\r\n\r\nSummary\r\nLlamaIndex and LangChain are two frameworks for building LLM applications. While LlamaIndex focuses on RAG use cases, LangChain seems more widely adopted. But how do they differ in practice? In this post, I compared the two frameworks in completing four common tasks:\r\n\r\nConnecting to a local LLM instance and build a chatbot.\r\nIndexing local files and building a RAG system.\r\nCombining the two above and making a chatbot with RAG capabilities.\r\nConverting the chatbot into an agent, so that it may use more tools and do simple reasoning.\r\nI hope they help you make an informed choice for your LLM application. Also, good luck with your journey building your own chatbot!", "start_char_idx": 15417, "end_char_idx": 17224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"e6eb5348-d59b-42fa-89fc-572b46e016d2": {"node_ids": ["1ce4343f-6d9e-4ad2-a022-43fb350e9762", "24ae07c8-5092-4deb-a4d8-ec80b6e9964d", "e53633a3-f9a4-4305-857c-6411e1abccc3", "2fcd423d-6a44-4356-bc7d-c89f4a205298", "71d7c476-0afe-40a1-a3c4-a3fda89c9284", "e488104f-b966-4ebb-b47e-28a934adab0b", "3028c46c-bf70-451a-af78-07dc4ff01092", "fbe46a8d-a029-42ff-8a1b-e7856667517e", "d1f8c92f-a7df-4517-96dd-b1ecb792edee", "fb0b2d25-ff0f-4b9a-ac0b-94d3df42dc27", "1fdad8eb-2931-4437-9b07-94872518ca75", "f45c7b04-5b42-4598-97df-c227cf12ed14", "311de25e-191a-487a-9409-5fcbe975499a"], "metadata": {"file_path": "C:\\Users\\santo\\OneDrive\\Desktop\\Projects\\Twitter Production\\Data\\Langchain vs llmaindex.txt", "file_name": "Langchain vs llmaindex.txt", "file_type": "text/plain", "file_size": 17346, "creation_date": "2024-06-08", "last_modified_date": "2024-06-08"}}}}